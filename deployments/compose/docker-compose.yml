# =============================================================================
# Glyphoxa — Docker Compose
# =============================================================================
#
# Two ways to run:
#
#   Default (glyphoxa + postgres):
#     docker compose up
#
#   Local LLM stack (adds ollama, whisper, coqui-tts):
#     docker compose --profile local up
#
#   Local stack with local config (no cloud API keys required):
#     cp config.local.yaml config.yaml
#     docker compose --profile local up
#
# See README.md for full setup instructions.
# =============================================================================

services:
  # ---------------------------------------------------------------------------
  # PostgreSQL with pgvector extension
  # ---------------------------------------------------------------------------
  postgres:
    image: pgvector/pgvector:pg17
    ports:
      - "5432:5432"
    environment:
      POSTGRES_USER: glyphoxa
      POSTGRES_PASSWORD: glyphoxa
      POSTGRES_DB: glyphoxa
    volumes:
      - pgdata:/var/lib/postgresql/data
      - ./init.sql:/docker-entrypoint-initdb.d/init.sql:ro
    shm_size: 256m
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U glyphoxa -d glyphoxa"]
      interval: 5s
      timeout: 3s
      retries: 5
    restart: unless-stopped

  # ---------------------------------------------------------------------------
  # Glyphoxa application server
  # ---------------------------------------------------------------------------
  glyphoxa:
    build:
      context: ../..
      dockerfile: deployments/compose/Dockerfile
    ports:
      - "8080:8080"
    volumes:
      - ./config.yaml:/etc/glyphoxa/config.yaml:ro
    command: ["-config", "/etc/glyphoxa/config.yaml"]
    depends_on:
      postgres:
        condition: service_healthy
    restart: unless-stopped

  # ---------------------------------------------------------------------------
  # Ollama — local LLM inference server
  # ---------------------------------------------------------------------------
  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    profiles:
      - local
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 15s
    restart: unless-stopped

  # ---------------------------------------------------------------------------
  # Ollama model bootstrap (runs once)
  # ---------------------------------------------------------------------------
  ollama-bootstrap:
    image: ollama/ollama:latest
    profiles:
      - local
    depends_on:
      ollama:
        condition: service_healthy
    entrypoint: ["/bin/sh", "-c"]
    command:
      - |
        echo "Pulling LLM model..."
        ollama pull llama3.2
        echo "Pulling embedding model..."
        ollama pull nomic-embed-text
        echo "Models ready."
    environment:
      OLLAMA_HOST: "http://ollama:11434"
    restart: "no"

  # ---------------------------------------------------------------------------
  # Whisper.cpp — local speech-to-text server
  # Model (ggml-base.en.bin) is included in the image.
  # For GPU: swap image to ghcr.io/ggml-org/whisper.cpp:main-cuda
  # ---------------------------------------------------------------------------
  whisper:
    image: ghcr.io/ggml-org/whisper.cpp:main
    ports:
      - "9000:8080"
    profiles:
      - local
    entrypoint: ["/app/build/bin/whisper-server"]
    command: ["--host", "0.0.0.0", "--port", "8080", "--model", "/app/models/ggml-base.en.bin", "--convert"]
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:8080/ || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    restart: unless-stopped

  # ---------------------------------------------------------------------------
  # Coqui TTS — local text-to-speech server
  # Uses VITS model (fast, lightweight). For XTTS v2 (multilingual, higher
  # quality, ~2GB download), change to:
  #   tts_models/multilingual/multi-dataset/xtts_v2
  # For GPU: swap image to ghcr.io/coqui-ai/tts
  # ---------------------------------------------------------------------------
  tts:
    image: ghcr.io/coqui-ai/tts-cpu
    ports:
      - "5002:5002"
    profiles:
      - local
    entrypoint: ["tts-server"]
    command: ["--model_name", "tts_models/en/ljspeech/vits", "--port", "5002"]
    volumes:
      - tts_models:/root/.local/share/tts
    healthcheck:
      test: ["CMD", "python3", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:5002/')"]
      interval: 15s
      timeout: 10s
      retries: 5
      start_period: 60s
    restart: unless-stopped

volumes:
  pgdata:
  ollama_data:
  tts_models:
