# =============================================================================
# Glyphoxa — Docker Compose
# =============================================================================
#
# Two ways to run:
#
#   Default (glyphoxa + postgres):
#     docker compose up
#
#   Local LLM stack (adds ollama, coqui-tts):
#     docker compose --profile local up
#
#   Local stack with local config (no cloud API keys required):
#     cp config.local.yaml config.yaml
#     docker compose --profile local up
#
# Whisper.cpp is bundled directly into the Glyphoxa binary — no separate
# container needed. Model files are downloaded on first run by the
# whisper-model-download init service and stored in the whisper_models volume.
#
# See README.md for full setup instructions.
# =============================================================================

services:
  # ---------------------------------------------------------------------------
  # PostgreSQL with pgvector extension
  # ---------------------------------------------------------------------------
  postgres:
    image: pgvector/pgvector:pg17
    ports:
      - "5432:5432"
    environment:
      POSTGRES_USER: glyphoxa
      POSTGRES_PASSWORD: glyphoxa
      POSTGRES_DB: glyphoxa
    volumes:
      - pgdata:/var/lib/postgresql/data
      - ./init.sql:/docker-entrypoint-initdb.d/init.sql:ro
    shm_size: 256m
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U glyphoxa -d glyphoxa"]
      interval: 5s
      timeout: 3s
      retries: 5
    restart: unless-stopped

  # ---------------------------------------------------------------------------
  # Whisper model download (runs once)
  # Downloads the ggml model file if not already present.
  # ---------------------------------------------------------------------------
  whisper-model-download:
    image: curlimages/curl:latest
    user: root
    profiles:
      - local
    volumes:
      - whisper_models:/models
    entrypoint: ["/bin/sh", "-c"]
    command:
      - |
        MODEL_FILE="/models/ggml-base.en.bin"
        if [ -f "$${MODEL_FILE}" ]; then
          echo "Model already downloaded: $${MODEL_FILE}"
          exit 0
        fi
        echo "Downloading whisper model base.en..."
        curl -L -o "$${MODEL_FILE}" \
          "https://huggingface.co/ggerganov/whisper.cpp/resolve/main/ggml-base.en.bin"
        echo "Model downloaded: $(ls -lh $${MODEL_FILE})"
    restart: "no"

  # ---------------------------------------------------------------------------
  # Glyphoxa application server
  # Whisper.cpp is built into the binary — pass the model path via config.
  # ---------------------------------------------------------------------------
  glyphoxa:
    image: ghcr.io/mrwong99/glyphoxa:main
    ports:
      - "8080:8080"
    volumes:
      - ./config.yaml:/etc/glyphoxa/config.yaml:ro
      - whisper_models:/models:ro
    command: ["-config", "/etc/glyphoxa/config.yaml"]
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    depends_on:
      postgres:
        condition: service_healthy
    restart: unless-stopped

  # ---------------------------------------------------------------------------
  # Ollama — local LLM inference server
  # ---------------------------------------------------------------------------
  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    profiles:
      - local
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 15s
    restart: unless-stopped

  # ---------------------------------------------------------------------------
  # Ollama model bootstrap (runs once)
  # ---------------------------------------------------------------------------
  ollama-bootstrap:
    image: ollama/ollama:latest
    profiles:
      - local
    depends_on:
      ollama:
        condition: service_healthy
    entrypoint: ["/bin/sh", "-c"]
    command:
      - |
        echo "Pulling LLM model..."
        ollama pull llama3.2
        echo "Pulling embedding model..."
        ollama pull nomic-embed-text
        echo "Models ready."
    environment:
      OLLAMA_HOST: "http://ollama:11434"
    restart: "no"

  # ---------------------------------------------------------------------------
  # Coqui TTS — local text-to-speech server
  # Uses VITS model (fast, lightweight). For XTTS v2 (multilingual, higher
  # quality, ~2GB download), change to:
  #   tts_models/multilingual/multi-dataset/xtts_v2
  # For GPU: swap image to ghcr.io/coqui-ai/tts
  # ---------------------------------------------------------------------------
  tts:
    image: ghcr.io/coqui-ai/tts
    ports:
      - "5002:5002"
    profiles:
      - local
    entrypoint: ["tts-server"]
    command: ["--model_name", "tts_models/en/ljspeech/vits", "--port", "5002", "--use_cuda", "true"]
    volumes:
      - tts_models:/root/.local/share/tts
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "python3", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:5002/')"]
      interval: 15s
      timeout: 10s
      retries: 5
      start_period: 60s
    restart: unless-stopped

  # ---------------------------------------------------------------------------
  # Prometheus — metrics scraping
  # Enabled with: docker compose --profile alpha up
  # ---------------------------------------------------------------------------
  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    profiles:
      - alpha
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus_data:/prometheus
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"
      - "--storage.tsdb.retention.time=30d"
    restart: unless-stopped

  # ---------------------------------------------------------------------------
  # Grafana — dashboards and alerting
  # Enabled with: docker compose --profile alpha up
  # Default login: admin / admin
  # ---------------------------------------------------------------------------
  grafana:
    image: grafana/grafana:latest
    ports:
      - "3000:3000"
    profiles:
      - alpha
    environment:
      GF_SECURITY_ADMIN_PASSWORD: admin
      GF_USERS_ALLOW_SIGN_UP: "false"
    volumes:
      - grafana_data:/var/lib/grafana
      - ./grafana/provisioning:/etc/grafana/provisioning:ro
      - ./grafana/dashboards:/var/lib/grafana/dashboards:ro
    depends_on:
      - prometheus
    restart: unless-stopped

volumes:
  pgdata:
  ollama_data:
  tts_models:
  whisper_models:
  prometheus_data:
  grafana_data:
