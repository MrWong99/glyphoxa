# =============================================================================
# Glyphoxa — Docker Compose
# =============================================================================
#
# Two ways to run:
#
#   Default (glyphoxa + postgres):
#     docker compose up
#
#   Local LLM stack (adds ollama, coqui-tts):
#     docker compose --profile local up
#
#   Local stack with local config (no cloud API keys required):
#     cp config.local.yaml config.yaml
#     docker compose --profile local up
#
# Whisper.cpp is bundled directly into the Glyphoxa binary — no separate
# container needed. Model files are downloaded on first run by the
# whisper-model-download init service and stored in the whisper_models volume.
#
# See README.md for full setup instructions.
# =============================================================================

services:
  # ---------------------------------------------------------------------------
  # PostgreSQL with pgvector extension
  # ---------------------------------------------------------------------------
  postgres:
    image: pgvector/pgvector:pg17
    ports:
      - "5432:5432"
    environment:
      POSTGRES_USER: glyphoxa
      POSTGRES_PASSWORD: glyphoxa
      POSTGRES_DB: glyphoxa
    volumes:
      - pgdata:/var/lib/postgresql/data
      - ./init.sql:/docker-entrypoint-initdb.d/init.sql:ro
    shm_size: 256m
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U glyphoxa -d glyphoxa"]
      interval: 5s
      timeout: 3s
      retries: 5
    restart: unless-stopped

  # ---------------------------------------------------------------------------
  # Whisper model download (runs once)
  # Downloads the ggml model file if not already present.
  # ---------------------------------------------------------------------------
  whisper-model-download:
    image: curlimages/curl:latest
    profiles:
      - local
    volumes:
      - whisper_models:/models
    entrypoint: ["/bin/sh", "-c"]
    command:
      - |
        MODEL_FILE="/models/ggml-base.en.bin"
        if [ -f "$MODEL_FILE" ]; then
          echo "Model already downloaded: $MODEL_FILE"
          exit 0
        fi
        echo "Downloading whisper model base.en..."
        curl -L -o "$MODEL_FILE" \
          "https://huggingface.co/ggerganov/whisper.cpp/resolve/main/ggml-base.en.bin"
        echo "Model downloaded: $(ls -lh $MODEL_FILE)"
    restart: "no"

  # ---------------------------------------------------------------------------
  # Glyphoxa application server
  # Whisper.cpp is built into the binary — pass the model path via config.
  # ---------------------------------------------------------------------------
  glyphoxa:
    build:
      context: ../..
      dockerfile: deployments/compose/Dockerfile
    ports:
      - "8080:8080"
    volumes:
      - ./config.yaml:/etc/glyphoxa/config.yaml:ro
      - whisper_models:/models:ro
    command: ["-config", "/etc/glyphoxa/config.yaml"]
    depends_on:
      postgres:
        condition: service_healthy
    restart: unless-stopped

  # ---------------------------------------------------------------------------
  # Ollama — local LLM inference server
  # ---------------------------------------------------------------------------
  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    profiles:
      - local
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 15s
    restart: unless-stopped

  # ---------------------------------------------------------------------------
  # Ollama model bootstrap (runs once)
  # ---------------------------------------------------------------------------
  ollama-bootstrap:
    image: ollama/ollama:latest
    profiles:
      - local
    depends_on:
      ollama:
        condition: service_healthy
    entrypoint: ["/bin/sh", "-c"]
    command:
      - |
        echo "Pulling LLM model..."
        ollama pull llama3.2
        echo "Pulling embedding model..."
        ollama pull nomic-embed-text
        echo "Models ready."
    environment:
      OLLAMA_HOST: "http://ollama:11434"
    restart: "no"

  # ---------------------------------------------------------------------------
  # Coqui TTS — local text-to-speech server
  # Uses VITS model (fast, lightweight). For XTTS v2 (multilingual, higher
  # quality, ~2GB download), change to:
  #   tts_models/multilingual/multi-dataset/xtts_v2
  # For GPU: swap image to ghcr.io/coqui-ai/tts
  # ---------------------------------------------------------------------------
  tts:
    image: ghcr.io/coqui-ai/tts-cpu
    ports:
      - "5002:5002"
    profiles:
      - local
    entrypoint: ["tts-server"]
    command: ["--model_name", "tts_models/en/ljspeech/vits", "--port", "5002"]
    volumes:
      - tts_models:/root/.local/share/tts
    healthcheck:
      test: ["CMD", "python3", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:5002/')"]
      interval: 15s
      timeout: 10s
      retries: 5
      start_period: 60s
    restart: unless-stopped

volumes:
  pgdata:
  ollama_data:
  tts_models:
  whisper_models:
