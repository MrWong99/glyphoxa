# Glyphoxa â€” Docker Compose configuration
# For the local stack: cp config.local.yaml config.yaml
#
# Whisper.cpp runs natively inside Glyphoxa (no separate container).
# Mount model files to /models/ via the whisper_models volume.

server:
  listen_addr: ":8080"
  log_level: info

providers:
  llm:
    name: ollama
    base_url: "http://ollama:11434"
    model: llama3.2

  stt:
    name: whisper-native
    model: /models/ggml-base.en.bin
    options:
      language: en

  tts:
    name: coqui
    base_url: "http://tts:5002"
    model: tts_models/en/ljspeech/vits
    options:
      api_mode: standard
      language: en

  embeddings:
    name: ollama
    base_url: "http://ollama:11434"
    model: nomic-embed-text

  vad:
    name: silero
    options:
      frame_size_ms: 30
      speech_threshold: 0.5
      silence_threshold: 0.35

npcs: []

memory:
  postgres_dsn: postgres://glyphoxa:glyphoxa@postgres:5432/glyphoxa?sslmode=disable
  embedding_dimensions: 768

mcp:
  servers: []
