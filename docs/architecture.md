---
nav_order: 2
---

# ğŸ—ï¸ Architecture Overview

## ğŸ¯ System Overview

Glyphoxa is a real-time voice AI framework that brings AI-driven talking NPCs into live TTRPG voice chat sessions. It captures player speech from a voice channel (Discord, WebRTC), routes it through a streaming AI pipeline (STT, LLM, TTS â€” or a single speech-to-speech model), and plays back NPC dialogue with distinct voices, personalities, and persistent memory â€” all within a 1.2-second mouth-to-ear latency target.

Written in Go for native concurrency, every pipeline stage runs as a goroutine connected by channels, enabling true end-to-end streaming where TTS starts synthesising before the LLM finishes generating.

---

## ğŸ—ºï¸ Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          Audio Transport Layer                            â”‚
â”‚                    (Discord / WebRTC / Custom Platform)                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   Audio In            â”‚                Audio Out                          â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚   â”‚ Per-Speaker    â”‚   â”‚   â”‚ Audio Mixer                              â”‚    â”‚
â”‚   â”‚ Streams        â”‚   â”‚   â”‚  - Priority queue (addressed NPC first)  â”‚    â”‚
â”‚   â”‚ (chan AudioFr) â”‚   â”‚   â”‚  - Barge-in detection (truncate on VAD)  â”‚    â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚  - Natural pacing (200-500ms gaps)       â”‚    â”‚
â”‚           â”‚           â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚           â”‚     Agent Orchestrator + Router       â”‚                       â”‚
â”‚           â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚                       â”‚
â”‚           â”‚   â”‚  Address Detection          â”‚     â”‚                       â”‚
â”‚           â”‚   â”‚  Turn-Taking / Barge-in     â”‚     â”‚                       â”‚
â”‚           â”‚   â”‚  DM Commands & Puppet Mode  â”‚     â”‚                       â”‚
â”‚           â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚                       â”‚
â”‚           â”‚              â”‚                        â”‚                       â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚                       â”‚
â”‚    â”‚   NPC #1    â”‚ â”‚   NPC #2   â”‚ â”‚  NPC #3  â”‚ ...                       â”‚
â”‚    â”‚  (Agent)    â”‚ â”‚  (Agent)   â”‚ â”‚ (Agent)  â”‚   â”‚                       â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜   â”‚                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                          Voice Engines                                    â”‚
â”‚                                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Cascaded Engine        â”‚ â”‚  S2S Engine   â”‚ â”‚  Sentence Cascade     â”‚  â”‚
â”‚  â”‚  STT â†’ LLM â†’ TTS       â”‚ â”‚  Gemini Live  â”‚ â”‚  âš ï¸ Experimental      â”‚  â”‚
â”‚  â”‚  (full pipeline)        â”‚ â”‚  OpenAI RT    â”‚ â”‚  Fast+Strong models   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   Memory Subsystem               â”‚    MCP Tool Execution                 â”‚
â”‚                                  â”‚                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚  L1   â”‚ â”‚   L2   â”‚ â”‚  L3  â”‚  â”‚  â”‚ Dice  â”‚ â”‚ Rules â”‚ â”‚ Memory â”‚      â”‚
â”‚  â”‚Sessionâ”‚ â”‚Semanticâ”‚ â”‚Graph â”‚  â”‚  â”‚Roller â”‚ â”‚Lookup â”‚ â”‚ Query  â”‚ ...  â”‚
â”‚  â”‚  Log  â”‚ â”‚ Index  â”‚ â”‚(KG)  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜  â”‚  Budget Tiers: instant/fast/standard  â”‚
â”‚  â”€â”€â”€ all on PostgreSQL â”€â”€â”€â”€â”€â”€â”€  â”‚                                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   Observability (OpenTelemetry)  â”‚  Resilience (Fallback + Circuit Break)â”‚
â”‚   Metrics Â· Traces Â· Middleware  â”‚  LLM / STT / TTS provider failover   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”€ Data Flow

The voice pipeline is a streaming chain. Each stage is a goroutine reading from an input channel and writing to an output channel. Stages overlap â€” TTS starts before the LLM finishes, audio playback starts before TTS finishes.

### Cascaded Path (default)

```
 Player speaks
      â”‚
      â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   50-100ms    Local Silero VAD, no network hop
 â”‚    VAD   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  Segments speech from silence
 â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
      â”‚
      â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   200-300ms   Deepgram streaming / whisper.cpp
 â”‚   STT    â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  Keyword boost from knowledge graph
 â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜               Phonetic entity correction on final
      â”‚
      â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ Speculative Memory Pre-fetch (parallel)
      â”‚                    Vector search starts on STT partials
      â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   30-50ms    In-memory graph + recent transcript
 â”‚ Hot Ctx  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  NPC identity, scene, relationships
 â”‚ Assembly â”‚
 â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
      â”‚
      â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   300-500ms   GPT-4o-mini / Claude / Gemini Flash
 â”‚   LLM    â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  Streaming tokens via Go channel
 â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜               MCP tool calls execute inline
      â”‚
      â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   75-150ms    ElevenLabs Flash / Coqui XTTS
 â”‚   TTS    â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  Sentence-by-sentence as tokens arrive
 â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
      â”‚
      â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   20-50ms     Opus encoding + platform transport
 â”‚  Mixer   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  Priority queue, barge-in, pacing
 â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
      â”‚
      â–¼
 NPC voice plays
                â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                Total: 650-1100ms (pipelined)
```

### S2S Path (speech-to-speech)

Audio goes directly to the S2S provider (Gemini Live or OpenAI Realtime), which handles recognition, generation, and synthesis in a single API call. Audio streams back through the same mixer and transport layer.

**Latency:** 150-600ms first audio, depending on provider.

### Memory Write-back (shared)

After both paths, the complete exchange (player utterance + NPC response) is written to the session transcript (L1). A background goroutine runs phonetic correction and entity extraction for the knowledge graph (L3).

---

## ğŸ“¦ Key Packages

### Application Layer (`cmd/` and `internal/`)

| Package | Location | Responsibility |
|---------|----------|----------------|
| `cmd/glyphoxa` | `cmd/glyphoxa/` | Entry point. Parses flags, loads config, wires providers, starts the app, handles signals (SIGINT/SIGTERM). |
| `internal/app` | `internal/app/` | Top-level wiring. Creates and connects all subsystems via functional options. Owns `App.Run()` lifecycle and `SessionManager` for multi-guild sessions. |
| `internal/agent` | `internal/agent/` | `NPCAgent` and `Router` interfaces. NPC identity, scene context, utterance handling. Sub-packages: `orchestrator` (address detection, turn-taking, utterance buffering), `npcstore` (PostgreSQL-backed NPC definitions). |
| `internal/engine` | `internal/engine/` | `VoiceEngine` interface â€” the core abstraction over the conversational loop. Sub-packages: `cascade` (STTâ†’LLMâ†’TTS pipeline), `s2s` (Gemini Live / OpenAI Realtime wrapper). |
| `internal/config` | `internal/config/` | Configuration schema, YAML loader, environment variable overlay, provider registry, file watcher for hot-reload, and config diffing. |
| `internal/discord` | `internal/discord/` | Discord bot layer. Slash command router, interaction handlers (`/npc`, `/session`, `/entity`, `/campaign`, `/recap`, `/feedback`), DM role permissions, voice command filtering, pipeline stats dashboard. |
| `internal/mcp` | `internal/mcp/` | MCP host interface and implementation. Tool registry with budget tiers, latency calibration, LLM-to-MCP bridge. Built-in tools: dice roller, rules lookup, memory query, file I/O. |
| `internal/session` | `internal/session/` | Session lifecycle management. Context window tracking with auto-summarisation, memory guard (L1 write-through), reconnection handling, transcript consolidation. |
| `internal/hotctx` | `internal/hotctx/` | Hot context assembly and formatting. Concurrent fetch of NPC identity (L3), recent transcript (L1), and scene context. Speculative memory pre-fetch on STT partials. Target: <50ms. |
| `internal/observe` | `internal/observe/` | OpenTelemetry metrics (Prometheus exporter), distributed tracing, HTTP middleware for latency/status instrumentation, per-provider metric recording. |
| `internal/entity` | `internal/entity/` | Entity management: CRUD operations, YAML campaign import, VTT import (Foundry VTT, Roll20), in-memory store. |
| `internal/transcript` | `internal/transcript/` | Transcript correction pipeline. Phonetic matching against known entity names, LLM-based correction for low-confidence segments, verification. |
| `internal/resilience` | `internal/resilience/` | Provider failover with circuit breakers. `LLMFallback`, `STTFallback`, `TTSFallback` â€” each wraps multiple backends and auto-switches on failure. |
| `internal/health` | `internal/health/` | HTTP health endpoints. `/healthz` (liveness) and `/readyz` (readiness with pluggable checkers). |
| `internal/feedback` | `internal/feedback/` | Closed-alpha feedback storage. Append-only JSON lines file store. |

### Public Libraries (`pkg/`)

| Package | Location | Responsibility |
|---------|----------|----------------|
| `pkg/audio` | `pkg/audio/` | `Platform` and `Connection` interfaces for voice channel connectivity. `AudioFrame` types, drain utilities. Sub-packages: `discord` (discordgo voice adapter, Opus encode/decode), `webrtc` (Pion-based WebRTC platform, signaling, transport), `mixer` (priority queue with barge-in, natural pacing, heap-based scheduling), `mock`. |
| `pkg/memory` | `pkg/memory/` | Three-layer memory interfaces: `SessionStore` (L1), `SemanticIndex` (L2), `KnowledgeGraph` / `GraphRAGQuerier` (L3). Query options, schema SQL. Sub-packages: `postgres` (pgx/pgvector implementation, knowledge graph with recursive CTEs, semantic index), `mock`. |
| `pkg/provider` | `pkg/provider/` | Provider interfaces and implementations for all external AI services. Sub-packages by capability: `llm` (Provider interface + any-llm-go adapter), `stt` (Provider interface + Deepgram, whisper.cpp), `tts` (Provider interface + ElevenLabs, Coqui XTTS), `s2s` (Provider interface + Gemini Live, OpenAI Realtime), `vad` (Engine interface + Silero), `embeddings` (Provider interface + OpenAI, Ollama). Each has a `mock` sub-package. |

---

## ğŸ§© Interface-First Design

Every subsystem in Glyphoxa defines a Go interface as its public contract. Concrete implementations satisfy the interface, and compile-time assertions verify correctness at build time.

### The Pattern

```go
// 1. Define the interface (in a "types" or package-level file)
type Provider interface {
    StreamCompletion(ctx context.Context, req CompletionRequest) (<-chan Chunk, error)
    Complete(ctx context.Context, req CompletionRequest) (*CompletionResponse, error)
    CountTokens(messages []Message) (int, error)
    Capabilities() ModelCapabilities
}

// 2. Implement it (in a provider-specific package)
type AnyLLMProvider struct { /* ... */ }

// 3. Compile-time assertion (top of the implementation file)
var _ llm.Provider = (*AnyLLMProvider)(nil)
```

The `var _ Interface = (*Impl)(nil)` line ensures the compiler checks that `*Impl` satisfies `Interface` at build time, catching missing methods before any test runs.

### Where This Pattern Appears

| Interface | Package | Implementations |
|-----------|---------|-----------------|
| `audio.Platform` | `pkg/audio` | `discord.Platform`, `webrtc.Platform` |
| `audio.Connection` | `pkg/audio` | `discord.Connection`, `webrtc.Connection` |
| `engine.VoiceEngine` | `internal/engine` | `cascade.Engine`, `s2s.Engine`, `mock.VoiceEngine` |
| `llm.Provider` | `pkg/provider/llm` | `anyllm.Provider`, `resilience.LLMFallback`, `mock.Provider` |
| `stt.Provider` | `pkg/provider/stt` | `deepgram.Provider`, `whisper.Provider`, `whisper.NativeProvider`, `resilience.STTFallback`, `mock.Provider` |
| `tts.Provider` | `pkg/provider/tts` | `elevenlabs.Provider`, `coqui.Provider`, `resilience.TTSFallback`, `mock.Provider` |
| `s2s.Provider` | `pkg/provider/s2s` | `gemini.Provider`, `openai.Provider`, `mock.Provider` |
| `vad.Engine` | `pkg/provider/vad` | `mock.Engine` (Silero via silero-vad-go) |
| `embeddings.Provider` | `pkg/provider/embeddings` | `openai.Provider`, `ollama.Provider`, `mock.Provider` |
| `memory.SessionStore` | `pkg/memory` | `postgres.Store`, `session.MemoryGuard`, `mock.Store` |
| `memory.KnowledgeGraph` | `pkg/memory` | `postgres.KnowledgeGraph`, `mock.Store` |
| `mcp.Host` | `internal/mcp` | `mcphost.Host`, `mock.Host` |
| `agent.NPCAgent` | `internal/agent` | `agent.NPC`, `mock.NPCAgent` |

This design means swapping any provider (e.g., replacing ElevenLabs with Coqui XTTS, or Deepgram with whisper.cpp) is a configuration change â€” the orchestrator never imports a concrete provider package.

---

## â±ï¸ Latency Budget

The hard constraint is **< 1.2 seconds mouth-to-ear** (player finishes speaking to NPC voice starts playing). The hard limit is 2.0 seconds.

### Cascaded Pipeline Breakdown

| Stage | Budget | Hard Limit | Technique |
|-------|--------|------------|-----------|
| VAD + silence detection | 50â€“100ms | â€” | Local Silero VAD. No network hop. Sub-ms inference. |
| STT (streaming final) | 200â€“300ms | 500ms | Deepgram streaming. Transcript ready ~200ms after speech ends. |
| Speculative pre-fetch | 0ms (overlapped) | â€” | Vector search + graph query fires on STT partials, in parallel. |
| Hot context assembly | 30â€“50ms | 150ms | In-memory graph traversal + recent transcript slice. |
| LLM time-to-first-token | 300â€“500ms | 800ms | GPT-4o-mini or Gemini Flash streaming. |
| TTS time-to-first-byte | 75â€“150ms | 500ms | ElevenLabs Flash v2.5 streaming. |
| Audio transport overhead | 20â€“50ms | â€” | Opus encoding + platform playback. |
| **Total (pipelined)** | **650â€“1100ms** | **2000ms** | Pipelining overlaps STT tail with pre-fetch, LLM streaming with TTS streaming. |

### S2S Comparison

| Engine | First Audio | Trade-offs |
|--------|-------------|------------|
| Cascaded (pipelined) | 650â€“1100ms | Full control over voice, model, tools |
| OpenAI Realtime (mini) | 150â€“400ms | Limited voices, 32k context window |
| OpenAI Realtime (full) | 200â€“500ms | Better quality, higher cost |
| Gemini Live (flash) | 300â€“600ms | 128k context, session resumption, free tier |

### Why Streaming is Non-Negotiable

Without end-to-end streaming, latencies would be additive rather than overlapping:

```
Sequential:  VAD(100) + STT(300) + LLM(500) + TTS(200) + Transport(50) = 1150ms minimum
                                                                          (often 1500-2000ms)

Pipelined:   VAD(100) + STT(200) â”€â”€â”
                                    â”œâ”€â”€ LLM TTFT(400) â”€â”€â”
             Pre-fetch(parallel) â”€â”€â”˜                     â”œâ”€â”€ TTS TTFB(100) + Transport(50)
                                                         â””â”€â”€ Total: ~850ms typical
```

Go's channel-based concurrency makes this natural: each stage is a goroutine reading from an input channel and writing to an output channel.

---

## ğŸ™ï¸ Engine Types

Each NPC declares its engine type in configuration. The `VoiceEngine` interface unifies all types so the orchestrator is engine-agnostic.

### Cascaded Engine (`cascade.Engine`)

The full **STT â†’ LLM â†’ TTS** pipeline. Each stage is a separate provider, giving maximum flexibility:

- **Use when:** You need distinct voices (ElevenLabs voice cloning), specific LLM choice (Claude for reasoning, GPT-4o-mini for speed), tool calling, or fine-grained control.
- **Latency:** 650â€“1100ms first audio.
- **Location:** `internal/engine/cascade/`

### S2S Engine (`s2s.Engine`)

A single API call handles audio-in to audio-out. Wraps **Gemini Live** or **OpenAI Realtime**.

- **Use when:** Lowest latency is the priority and you can accept the provider's built-in voices and limited tool support.
- **Latency:** 150â€“600ms first audio.
- **Location:** `internal/engine/s2s/`

### Sentence Cascade (experimental)

A dual-model approach: a **fast model** (GPT-4o-mini) generates the opening sentence for immediate TTS playback (~500ms), while a **strong model** (Claude Sonnet) generates the substantive continuation in parallel. The listener hears a single continuous utterance.

- **Use when:** You want perceived sub-600ms latency with the quality of a strong model.
- **Status:** Experimental. See [design/05-sentence-cascade.md](design/05-sentence-cascade.md).
- **Location:** `internal/engine/cascade/` (built as a mode of the cascade engine)

| Engine | First Audio | Voice Control | Tool Calling | Context Window |
|--------|-------------|---------------|--------------|----------------|
| Cascaded | 650â€“1100ms | Full (any TTS provider) | Full (MCP budget tiers) | Provider-dependent |
| S2S (Gemini) | 300â€“600ms | Provider voices only | Limited | 128k tokens |
| S2S (OpenAI) | 150â€“500ms | Provider voices only | Limited | 32k tokens |
| Sentence Cascade | ~500ms perceived | Full | Full | Provider-dependent |

---

## ğŸ“š Design Documents

The full design is captured in a series of detailed documents. This architecture overview is the starting point; each design doc goes deep on its topic.

| # | Document | Description |
|---|----------|-------------|
| 00 | [Overview](design/00-overview.md) | Vision, product principles, core capabilities, performance targets |
| 01 | [Architecture](design/01-architecture.md) | System layers, detailed data flow, audio mixing, streaming requirements |
| 02 | [Providers](design/02-providers.md) | LLM, STT, TTS, S2S, Audio platform interfaces and provider trade-offs |
| 03 | [Memory](design/03-memory.md) | Three-layer hybrid memory: session log, semantic index, knowledge graph |
| 04 | [MCP Tools](design/04-mcp-tools.md) | Tool integration, budget tiers, built-in tools, performance constraints |
| 05 | [Sentence Cascade](design/05-sentence-cascade.md) | Experimental dual-model cascade for perceived sub-600ms latency |
| 06 | [NPC Agents](design/06-npc-agents.md) | Agent design, multi-NPC orchestration, address detection, turn-taking |
| 07 | [Technology](design/07-technology.md) | Why Go, dependency stack, CGo decisions, latency budget breakdown |
| 08 | [Open Questions](design/08-open-questions.md) | Unresolved design questions and decisions in progress |
| 09 | [Roadmap](design/09-roadmap.md) | Development phases and milestone planning |
| 10 | [Knowledge Graph](design/10-knowledge-graph.md) | L3 graph schema, PostgreSQL adjacency tables, recursive CTEs, GraphRAG |
| â€” | [To Be Discussed](design/to-be-discussed.md) | Items pending team discussion |

---

## ğŸ‘€ See Also

- [getting-started.md](getting-started.md) â€” Setup, build, and first run guide
- [providers.md](providers.md) â€” Provider configuration and swapping guide
- [audio-pipeline.md](audio-pipeline.md) â€” Deep dive into audio transport, mixing, and barge-in
- [memory.md](memory.md) â€” Memory system usage, hot context, and knowledge graph queries
- [design/00-overview.md](design/00-overview.md) â€” Vision and product principles
- [design/01-architecture.md](design/01-architecture.md) â€” Detailed system architecture
- [design/07-technology.md](design/07-technology.md) â€” Technology decisions and dependency stack
- [CONTRIBUTING.md](https://github.com/MrWong99/glyphoxa/blob/main/CONTRIBUTING.md) â€” Development setup, code style, and workflow
